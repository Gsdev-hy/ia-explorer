
import React from 'react';
import Hero from '@/components/Hero';
import SectionHeading from '@/components/SectionHeading';
import { Card, CardContent } from '@/components/ui/card';
import { Brain, BookOpen, MessageSquare, Zap, Target, Lightbulb, Network, Code, Layers } from 'lucide-react';
import LessonSection from '@/components/courses/LessonSection';
import CodeExample from '@/components/courses/CodeExample';
import ZoomOn from '@/components/courses/ZoomOn';
import Illustration from '@/components/courses/Illustration';
import CourseQuiz, { QuizQuestion } from '@/components/courses/CourseQuiz';
import BackToResourcesButton from '@/components/courses/BackToResourcesButton';
import DidYouKnow from '@/components/courses/DidYouKnow';
import AnalogyBox from '@/components/courses/AnalogyBox';
import TechnicalTooltip from '@/components/courses/TechnicalTooltip';
import StatsGrid from '@/components/courses/StatsGrid';
import CourseHeader from '@/components/courses/CourseHeader';
import CourseModule from '@/components/courses/CourseModule';
import CourseConclusion from '@/components/courses/CourseConclusion';
import ConceptCard from '@/components/courses/nlp/ConceptCard';
import ArchitectureComparison from '@/components/courses/nlp/ArchitectureComparison';
import InteractiveNLPDemo from '@/components/courses/nlp/InteractiveNLPDemo';

/**
 * Page du cours sur le NLP et les LLM
 * @returns {JSX.Element} Le composant de la page
 */
const NLPComprehension = () => {
  const quizQuestions: QuizQuestion[] = [
    {
      question: "Qu'est-ce qui distingue principalement les Transformers des RNN traditionnels ?",
      options: [
        "Les Transformers utilisent uniquement des couches de convolution",
        "Les Transformers traitent les s√©quences en parall√®le gr√¢ce au m√©canisme d'attention",
        "Les Transformers ne peuvent traiter que des textes courts",
        "Les Transformers n'utilisent pas de r√©seaux de neurones"
      ],
      correctAnswer: 1,
      explanation: "Les Transformers r√©volutionnent le NLP en traitant toute la s√©quence en parall√®le gr√¢ce au m√©canisme d'attention, contrairement aux RNN qui traitent s√©quentiellement."
    },
    {
      question: "Que signifie 'BERT' et quelle est sa sp√©cificit√© ?",
      options: [
        "Basic Encoding Representation Transformer - sp√©cialis√© dans la g√©n√©ration",
        "Bidirectional Encoder Representations from Transformers - comprend le contexte dans les deux sens",
        "Bilateral Encoding Recursive Transformer - utilise la r√©cursion",
        "Binary Embedding Relational Transformer - traite uniquement du binaire"
      ],
      correctAnswer: 1,
      explanation: "BERT (Bidirectional Encoder Representations from Transformers) r√©volutionne la compr√©hension en analysant le contexte dans les deux directions, permettant une meilleure compr√©hension du sens."
    },
    {
      question: "Quelle est la diff√©rence principale entre GPT et BERT ?",
      options: [
        "GPT est bidirectionnel, BERT est unidirectionnel",
        "GPT g√©n√®re du texte (d√©codeur), BERT comprend le texte (encodeur)",
        "GPT utilise l'attention, BERT utilise la convolution",
        "Il n'y a aucune diff√©rence significative"
      ],
      correctAnswer: 1,
      explanation: "GPT est optimis√© pour la g√©n√©ration de texte (architecture d√©codeur), tandis que BERT excelle dans la compr√©hension et l'analyse (architecture encodeur)."
    },
    {
      question: "Qu'est-ce que le 'fine-tuning' dans le contexte des LLM ?",
      options: [
        "R√©duire la taille du mod√®le pour √©conomiser la m√©moire",
        "Ajuster un mod√®le pr√©-entra√Æn√© sur une t√¢che sp√©cifique",
        "Augmenter le nombre de param√®tres du mod√®le",
        "Nettoyer les donn√©es d'entra√Ænement"
      ],
      correctAnswer: 1,
      explanation: "Le fine-tuning consiste √† prendre un mod√®le d√©j√† entra√Æn√© sur une large base de donn√©es et √† l'affiner sur une t√¢che ou un domaine sp√©cifique pour am√©liorer ses performances."
    },
    {
      question: "Que repr√©sente le m√©canisme d'attention dans les Transformers ?",
      options: [
        "La capacit√© du mod√®le √† se concentrer sur les parties pertinentes de l'entr√©e",
        "Un syst√®me pour √©viter la surcharge computationnelle",
        "Une m√©thode pour compresser les donn√©es",
        "Un algorithme de tri des mots par importance"
      ],
      correctAnswer: 0,
      explanation: "L'attention permet au mod√®le de 'faire attention' aux parties les plus pertinentes de la s√©quence d'entr√©e pour chaque pr√©diction, comme nous le faisons naturellement en lisant."
    }
  ];

  const didYouKnowItems = [
    {
      title: "R√©volution Transformer",
      content: "L'architecture Transformer, introduite en 2017 avec le papier 'Attention Is All You Need', a r√©volutionn√© le NLP en rempla√ßant les RNN par des m√©canismes d'attention pure."
    },
    {
      title: "Puissance de GPT-3",
      content: "GPT-3 contient 175 milliards de param√®tres et peut r√©aliser des t√¢ches qu'il n'a jamais vues pendant l'entra√Ænement, d√©montrant des capacit√©s d'apprentissage en contexte impressionnantes."
    },
    {
      title: "Donn√©es d'entra√Ænement",
      content: "Les grands mod√®les de langage sont entra√Æn√©s sur des t√©raoctets de texte, incluant des livres, articles, sites web, repr√©sentant une grande partie de la connaissance humaine √©crite."
    }
  ];

  const statsData = [
    {
      value: "2017",
      description: "Ann√©e de l'invention des Transformers",
      bgGradient: "from-blue-50 to-blue-100 dark:from-blue-950/50 dark:to-blue-900/50"
    },
    {
      value: "175B",
      description: "Param√®tres dans GPT-3",
      bgGradient: "from-green-50 to-green-100 dark:from-green-950/50 dark:to-green-900/50"
    },
    {
      value: "100+",
      description: "Langues support√©es par les LLM modernes",
      bgGradient: "from-purple-50 to-purple-100 dark:from-purple-950/50 dark:to-purple-900/50"
    }
  ];

  const learningModules = [
    {
      icon: <Brain className="h-6 w-6" />,
      title: "Fondamentaux du NLP",
      items: [
        "Tokenisation et pr√©traitement",
        "Repr√©sentations vectorielles (embeddings)",
        "Architectures de base (RNN, LSTM)",
        "M√©triques d'√©valuation en NLP"
      ]
    },
    {
      icon: <Zap className="h-6 w-6" />,
      title: "L'√®re des Transformers",
      items: [
        "M√©canisme d'attention expliqu√©",
        "Architecture Transformer d√©taill√©e",
        "BERT vs GPT : comprendre les diff√©rences",
        "Fine-tuning et transfer learning"
      ]
    },
    {
      icon: <Target className="h-6 w-6" />,
      title: "Applications pratiques",
      items: [
        "Analyse de sentiment en temps r√©el",
        "Syst√®mes de questions-r√©ponses",
        "R√©sum√© automatique de texte",
        "G√©n√©ration de contenu cr√©atif"
      ]
    }
  ];

  return (
    <>
      <BackToResourcesButton />
      
      <Hero
        title="NLP et LLM : Comprendre le Traitement du Langage"
        subtitle="Explorez l'univers fascinant du traitement automatique du langage naturel, des bases th√©oriques aux mod√®les de langage les plus avanc√©s"
      />

      <section className="section-container mb-10">
        <CourseHeader
          title="NLP et LLM : Comprendre le Traitement du Langage"
          subtitle="Un voyage dans l'art de faire comprendre le langage humain aux machines"
          author="Dr. Sarah Dubois"
          authorDescription="Experte en NLP et Intelligence Artificielle, 12 ans de recherche en traitement du langage"
          duration="8-10 semaines (5-7h/semaine)"
          level="Interm√©diaire"
          audience="D√©veloppeurs, Data Scientists, Linguistes computationnels"
          tags={["NLP", "LLM", "Transformers", "BERT", "GPT", "Pratique"]}
        />

        <div className="max-w-4xl mx-auto mb-12">
          <div className="bg-gradient-to-r from-emerald-50 to-blue-50 dark:from-emerald-950/30 dark:to-blue-950/30 rounded-xl p-8 mb-8">
            <h3 className="text-2xl font-bold mb-4 text-primary">üó£Ô∏è Bienvenue dans l'art de la communication machine-humain !</h3>
            <p className="text-lg mb-4 text-foreground">
              Imaginez un monde o√π les machines comprennent non seulement nos mots, mais aussi nos √©motions, notre contexte, 
              et m√™me nos non-dits. C'est exactement ce que vous allez d√©couvrir dans ce voyage au c≈ìur du NLP !
            </p>
            <p className="mb-4 text-foreground">
              Du simple correcteur orthographique aux assistants virtuels en passant par la traduction instantan√©e, 
              le traitement du langage naturel transforme notre fa√ßon d'interagir avec la technologie. Aujourd'hui, 
              des mod√®les comme ChatGPT r√©volutionnent notre rapport √† l'information et √† la cr√©ativit√©.
            </p>
            <p className="font-medium text-primary">
              Ce cours vous guide pas √† pas, de la compr√©hension des m√©canismes fondamentaux jusqu'√† la ma√Ætrise 
              des architectures les plus avanc√©es. Pr√©parez-vous √† percer les myst√®res du langage artificiel !
            </p>
          </div>

          <AnalogyBox
            title="Le NLP, c'est comme apprendre une langue √©trang√®re"
            content="Quand vous apprenez une nouvelle langue, vous commencez par les mots (vocabulaire), puis les r√®gles (grammaire), et enfin le contexte (culture). Les machines font pareil : elles apprennent d'abord √† reconna√Ætre les mots, puis leurs relations, et enfin le sens global. La diff√©rence ? Elles peuvent traiter des millions de textes en quelques heures !"
            variant="info"
          />
        </div>

        <DidYouKnow items={didYouKnowItems} />

        <StatsGrid stats={statsData} />

        <CourseModule
          title="Programme du cours"
          description="Un parcours structur√© en 6 modules, de la th√©orie aux applications concr√®tes"
          modules={learningModules}
        />

        <div className="max-w-4xl mx-auto">
          {/* Module 1: Introduction au NLP */}
          <LessonSection title="Module 1 : Les Fondamentaux du Traitement du Langage" icon={<MessageSquare size={24} />} delay={1}>
            <p className="text-lg mb-4">
              Bienvenue dans l'univers passionnant du NLP ! Mais d'abord, une question simple : 
              comment faites-vous pour comprendre cette phrase que vous lisez en ce moment ?
            </p>
            
            <ZoomOn title="Le d√©fi de la compr√©hension automatique">
              <p className="mb-3">
                Pour nous, humains, comprendre le langage semble naturel. Mais pour une machine, c'est un d√©fi colossal :
              </p>
              <ul className="list-disc pl-6 space-y-2">
                <li><strong>Ambigu√Øt√© :</strong> "Il a vu l'homme avec des jumelles" - qui a les jumelles ?</li>
                <li><strong>Contexte :</strong> "√áa craint" peut √™tre positif ou n√©gatif selon la situation</li>
                <li><strong>R√©f√©rences :</strong> "Il est parti" - qui est "il" ?</li>
                <li><strong>Implicite :</strong> "Il fait chaud ici" peut signifier "ouvrez la fen√™tre"</li>
              </ul>
            </ZoomOn>

            <Illustration 
              src="https://images.unsplash.com/photo-1488590528505-98d2b5aba04b?ixlib=rb-4.0.3&auto=format&fit=crop&w=1500&q=80" 
              alt="Ordinateur portable affichant du code, repr√©sentant le traitement automatique du langage"
              caption="Le NLP transforme le langage humain en donn√©es que les machines peuvent comprendre et traiter"
              width="2/3"
            />

            <h4 className="text-xl font-semibold mt-6 mb-3">Les √©tapes du traitement du langage</h4>
            <p className="mb-4">
              Le NLP d√©compose la compr√©hension du langage en plusieurs √©tapes, comme un chef qui pr√©pare un plat complexe :
            </p>

            <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mt-6">
              <ConceptCard
                icon={<Layers className="h-5 w-5" />}
                title="Tokenisation"
                description="D√©couper le texte en unit√©s plus petites (mots, sous-mots)"
                examples={["'Hello world' ‚Üí ['Hello', 'world']", "'I'm' ‚Üí ['I', 'am'] ou ['I', \"'\", 'm']"]}
                color="blue"
              />
              <ConceptCard
                icon={<Brain className="h-5 w-5" />}
                title="Analyse syntaxique"
                description="Comprendre la structure grammaticale des phrases"
                examples={["Identifier sujet, verbe, compl√©ment", "D√©tecter les relations entre mots"]}
                color="green"
              />
              <ConceptCard
                icon={<Target className="h-5 w-5" />}
                title="Analyse s√©mantique"
                description="Extraire le sens et les concepts"
                examples={["D√©tection d'entit√©s (personnes, lieux)", "Relations entre concepts"]}
                color="purple"
              />
              <ConceptCard
                icon={<Lightbulb className="h-5 w-5" />}
                title="Compr√©hension pragmatique"
                description="Interpr√©ter selon le contexte et l'intention"
                examples={["D√©tection d'ironie", "Intentions cach√©es"]}
                color="orange"
              />
            </div>

            <CodeExample 
              title="Premier exemple : Analyse de sentiment simple"
              language="python"
              code={`import re
from collections import Counter

class AnalyseurSentiment:
    def __init__(self):
        # Dictionnaires de mots positifs et n√©gatifs
        self.mots_positifs = {
            'excellent', 'fantastique', 'g√©nial', 'super', 'parfait',
            'adore', 'aime', 'content', 'heureux', 'satisfait'
        }
        self.mots_negatifs = {
            'horrible', 'nul', 'd√©teste', 'mauvais', 'terrible',
            'd√©√ßu', 'triste', '√©nerv√©', 'frustr√©', 'catastrophique'
        }
    
    def nettoyer_texte(self, texte):
        """Nettoie et pr√©pare le texte pour l'analyse"""
        # Conversion en minuscules
        texte = texte.lower()
        # Suppression de la ponctuation
        texte = re.sub(r'[^\w\s]', '', texte)
        # Division en mots
        mots = texte.split()
        return mots
    
    def analyser_sentiment(self, texte):
        """Analyse le sentiment d'un texte"""
        mots = self.nettoyer_texte(texte)
        
        score_positif = sum(1 for mot in mots if mot in self.mots_positifs)
        score_negatif = sum(1 for mot in mots if mot in self.mots_negatifs)
        
        # Calcul du sentiment
        if score_positif > score_negatif:
            return "positif", score_positif / len(mots)
        elif score_negatif > score_positif:
            return "n√©gatif", score_negatif / len(mots)
        else:
            return "neutre", 0.0

# Exemple d'utilisation
analyseur = AnalyseurSentiment()
texte = "J'adore ce produit, il est fantastique !"
sentiment, confiance = analyseur.analyser_sentiment(texte)
print(f"Sentiment: {sentiment} (confiance: {confiance:.2f})")`}
              explanation="Ce code illustre les bases de l'analyse de sentiment avec une approche simple bas√©e sur des dictionnaires. Bien que basique, cette m√©thode montre les principes fondamentaux du NLP."
            />

            <AnalogyBox
              title="Les embeddings : donner une adresse aux mots"
              content="Imaginez que chaque mot de la langue fran√ßaise ait une adresse unique dans un espace multidimensionnel. Les mots similaires habitent dans le m√™me quartier ! 'Roi' et 'Reine' seraient voisins, tout comme 'Paris' et 'Londres'. C'est exactement ce que font les embeddings : ils donnent une 'adresse num√©rique' √† chaque mot."
              variant="tip"
            />
          </LessonSection>

          {/* Module 2: Architectures et Evolution */}
          <LessonSection title="Module 2 : L'√âvolution des Architectures NLP" icon={<Network size={24} />} delay={2}>
            <p className="text-lg mb-4">
              L'histoire du NLP ressemble √† l'√©volution des transports : des premiers pas h√©sitants aux 
              <TechnicalTooltip term="Transformer" definition="Architecture r√©volutionnaire bas√©e sur l'attention, qui traite les s√©quences en parall√®le">r√©volutions technologiques</TechnicalTooltip> 
              qui transforment tout !
            </p>

            <ZoomOn title="La r√©volution des Transformers : pourquoi tout a chang√© ?">
              <p className="mb-3">
                En 2017, le papier "Attention Is All You Need" r√©volutionne le NLP en une phrase : 
                "Et si on arr√™tait de traiter le texte mot par mot et qu'on regardait tout en m√™me temps ?"
              </p>
              <ul className="list-disc pl-6 space-y-2">
                <li><strong>Parall√©lisation :</strong> Plus besoin d'attendre le mot pr√©c√©dent pour traiter le suivant</li>
                <li><strong>Attention globale :</strong> Chaque mot peut "faire attention" √† tous les autres</li>
                <li><strong>Scalabilit√© :</strong> Plus de donn√©es = de meilleures performances</li>
                <li><strong>Transfer learning :</strong> Un mod√®le peut apprendre une t√¢che puis s'adapter √† d'autres</li>
              </ul>
            </ZoomOn>

            <h4 className="text-xl font-semibold mt-6 mb-4">Comparaison des architectures</h4>
            <ArchitectureComparison />

            <CodeExample 
              title="Comprendre l'attention : un m√©canisme simplifi√©"
              language="python"
              code={`import numpy as np

def attention_simplifiee(query, keys, values):
    """
    Impl√©mentation simplifi√©e du m√©canisme d'attention
    
    query: ce qu'on cherche √† comprendre
    keys: les √©l√©ments sur lesquels on peut se concentrer  
    values: l'information associ√©e √† chaque key
    """
    # √âtape 1: Calculer les scores d'attention
    scores = np.dot(query, keys.T)  # Produit scalaire
    
    # √âtape 2: Normaliser avec softmax
    attention_weights = np.exp(scores) / np.sum(np.exp(scores))
    
    # √âtape 3: Combiner les valeurs selon les poids
    output = np.dot(attention_weights, values)
    
    return output, attention_weights

# Exemple concret
# Phrase: "Le chat mange la souris"
# On veut comprendre "mange" en fonction du contexte

# Repr√©sentations simplifi√©es (dans la r√©alit√©, ce sont des vecteurs de 512+ dimensions)
mots = ["Le", "chat", "mange", "la", "souris"]
embeddings = np.array([
    [0.1, 0.2, 0.1],  # Le
    [0.8, 0.1, 0.2],  # chat (animal)
    [0.3, 0.9, 0.4],  # mange (action)
    [0.1, 0.1, 0.1],  # la
    [0.7, 0.2, 0.3]   # souris (animal)
])

# Query: on veut comprendre "mange"
query = embeddings[2]  # "mange"
keys = embeddings      # tous les mots
values = embeddings    # m√™mes valeurs pour simplifier

resultat, poids = attention_simplifiee(query, keys, values)

print("Poids d'attention pour 'mange':")
for mot, poids_mot in zip(mots, poids):
    print(f"{mot}: {poids_mot:.3f}")

# Le mod√®le fait attention surtout √† "chat" et "souris" 
# pour comprendre l'action "mange"`}
              explanation="Cette impl√©mentation simplifi√©e montre comment l'attention permet au mod√®le de se concentrer sur les parties pertinentes de la s√©quence. Dans la r√©alit√©, les Transformers utilisent une attention multi-t√™tes beaucoup plus sophistiqu√©e."
            />
          </LessonSection>

          {/* Module 3: LLM et Applications */}
          <LessonSection title="Module 3 : Les Grands Mod√®les de Langage (LLM)" icon={<Brain size={24} />} delay={3}>
            <p className="text-lg mb-4">
              Les LLM repr√©sentent l'aboutissement de d√©cennies de recherche en NLP. Mais comment un mod√®le 
              peut-il apprendre √† √©crire, traduire, r√©sumer et m√™me programmer ?
            </p>

            <ZoomOn title="Le secret des LLM : l'apprentissage sur Internet entier">
              <p className="mb-3">
                Les grands mod√®les de langage sont entra√Æn√©s sur des quantit√©s astronomiques de texte :
              </p>
              <ul className="list-disc pl-6 space-y-2">
                <li><strong>Livres et articles :</strong> Millions d'ouvrages num√©ris√©s</li>
                <li><strong>Pages web :</strong> Billions de pages index√©es</li>
                <li><strong>Forums et discussions :</strong> Conversations humaines r√©elles</li>
                <li><strong>Code source :</strong> Repositories GitHub publics</li>
              </ul>
              <p className="mt-3 text-sm text-muted-foreground">
                R√©sultat : ces mod√®les "connaissent" une grande partie de la connaissance humaine √©crite !
              </p>
            </ZoomOn>

            <h4 className="text-xl font-semibold mt-6 mb-4">D√©mo interactive : D√©couvrez le NLP en action</h4>
            <InteractiveNLPDemo />

            <div className="mt-8 p-6 bg-gradient-to-r from-blue-50 to-purple-50 dark:from-blue-950/30 dark:to-purple-950/30 rounded-xl">
              <h4 className="text-lg font-semibold mb-4 flex items-center gap-2">
                <Code className="h-5 w-5" />
                Applications r√©volutionnaires des LLM
              </h4>
              <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
                <div className="bg-white/80 dark:bg-gray-800/80 p-4 rounded-lg">
                  <h5 className="font-medium mb-2">üéØ Assistants conversationnels</h5>
                  <p className="text-sm text-muted-foreground">
                    ChatGPT, Claude, Bard : des IA capables de conversations naturelles sur tous les sujets
                  </p>
                </div>
                <div className="bg-white/80 dark:bg-gray-800/80 p-4 rounded-lg">
                  <h5 className="font-medium mb-2">üåç Traduction instantan√©e</h5>
                  <p className="text-sm text-muted-foreground">
                    DeepL, Google Translate : traduction de qualit√© professionnelle en temps r√©el
                  </p>
                </div>
                <div className="bg-white/80 dark:bg-gray-800/80 p-4 rounded-lg">
                  <h5 className="font-medium mb-2">‚úçÔ∏è G√©n√©ration de contenu</h5>
                  <p className="text-sm text-muted-foreground">
                    Articles, emails, code : cr√©ation automatique de contenu personnalis√©
                  </p>
                </div>
                <div className="bg-white/80 dark:bg-gray-800/80 p-4 rounded-lg">
                  <h5 className="font-medium mb-2">üîç Analyse intelligente</h5>
                  <p className="text-sm text-muted-foreground">
                    R√©sum√© de documents, extraction d'informations, analyse de sentiment
                  </p>
                </div>
              </div>
            </div>

            <AnalogyBox
              title="BERT vs GPT : L'√©l√®ve studieux vs le conteur cr√©atif"
              content="Imaginez deux √©tudiants : BERT lit attentivement tout un texte avant de r√©pondre aux questions (bidirectionnel), excellent pour comprendre. GPT lit de gauche √† droite et imagine la suite (unidirectionnel), excellent pour cr√©er. Chacun a sa sp√©cialit√©, comme un analyste vs un √©crivain !"
            />
          </LessonSection>

          {/* Quiz */}
          <div className="mt-16">
            <CourseQuiz 
              questions={quizQuestions}
              title="Quiz : Testez votre compr√©hension du NLP"
            />
          </div>

          {/* Conclusion */}
          <div className="mt-16">
            <CourseConclusion
              title="Bravo ! Vous ma√Ætrisez maintenant les fondamentaux du NLP et des LLM"
              description="Vous avez explor√© un domaine en pleine r√©volution. Voici vos nouvelles comp√©tences et comment continuer votre apprentissage."
              learningPoints={[
                "Compr√©hension des enjeux et d√©fis du traitement automatique du langage",
                "Ma√Ætrise des architectures cl√©s : RNN, LSTM, Transformers",
                "Connaissance des mod√®les phares : BERT, GPT et leurs applications",
                "Capacit√© √† analyser et choisir la bonne approche selon le probl√®me"
              ]}
              nextSteps={[
                "Exp√©rimentez avec les API d'OpenAI, Hugging Face ou Google",
                "Cr√©ez votre premier chatbot ou analyseur de sentiment",
                "Rejoignez la communaut√© NLP fran√ßaise sur Discord et LinkedIn",
                "Explorez les cours avanc√©s sur le fine-tuning et le RAG"
              ]}
            />
          </div>
        </div>
      </section>
    </>
  );
};

export default NLPComprehension;
